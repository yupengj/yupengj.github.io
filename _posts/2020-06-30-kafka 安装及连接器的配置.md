---
layout: post
title: "kafka 安装及连接器的配置"
date: 2020-06-30 20:00:00
tags: kafka
---

## 系统环境及软件版本

- window10 64位 系统
- kafka 2.3.0 版本(下载地址 http://kafka.apache.org/downloads)

本文章将介绍 kafka 安装、命令行方式的数据同步、 java 客户端方式的消息同步、kafka 连接器的配置和启动




## 启动并测试 kafka

- 下载 kafka 解压后目录结构
![](https://raw.githubusercontent.com/yupengj/yupengj.github.io/master/images/2020/kafka.png)

- 进入kafka文件夹下执行下面命令启动 ZooKeeper
```
bin/windows/zookeeper-server-start.bat config/zookeeper.properties
```

- 启动 kafak server
```
bin/windows/kafka-server-start.bat config/server.properties
```

- 创建 test 主题
```
bin/windows/kafka-topics.bat --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test
```

- 查看主题,控制台会输出 test
```
bin/windows/kafka-topics.bat --list --bootstrap-server localhost:9092 
```

### 创建 producer 和 consumer 测试消息同步

- 创建生成者客户端并输入测试消息。注 2.5版本后需要把 --broker-list 改为 --bootstrap-server
 ```
$ bin/windows/kafka-console-producer.bat --broker-list localhost:9092 --topic test
>nihao
>你好
```

- 创建消费者客户端接收消息。这里中文乱码暂时没有找到解决办法。
```
$ bin/windows/kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning
>nihao
>锟斤拷锟
```

- 继续在生产者客户端发送消息，消费者客户端会自动同步消息。

### java 客户端创建 producer 和 consumer 测试消息同步[java客户端源码地址](https://github.com/yupengj/kafka-examples)

- Producer 类，向 test 主题发送10条消息
```java
@Slf4j
public class Producer {
    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS_CONFIG);
        props.put(ProducerConfig.CLIENT_ID_CONFIG, "test_1");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);
        String topic = "test";
        int count = 0;
        while (true) {
            String messageKey = "key_" + (++count) + "你好";
            String messageValue = "value_" + count + "你好";
            kafkaProducer.send(new ProducerRecord<>(topic, messageKey, messageValue)).get();
            log.info("send message topic {} ( {}, {})", topic, messageKey, messageValue);
            if (count > 10) {//只发送10条消息
                break;
            }
        }
    }
}
```

-  Consumer 类，拉取 test 主题的消息，连续10次没有拉取到数据时结束轮询
```java
@Slf4j
public class Consumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaConfig.BOOTSTRAP_SERVERS_CONFIG);// kafka 集群
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "test_1"); // 消费组id
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "test_1"); // 消费客户端id
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // 从消息开始的位置读
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); // 不自动管理偏移量,即不记录消费者偏移量，可以重复读取数据方便测试
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

		final String topic = "test";
        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(props);
        kafkaConsumer.subscribe(Collections.singletonList(topic));
        long start = System.currentTimeMillis();
        int count = 0, num = 10;// 有10次拉取的数据记录为 0 时 结束轮询
        while (true) {
            ConsumerRecords<String, String> records = kafkaConsumer.poll(Duration.ofSeconds(1));
            for (ConsumerRecord<String, String> record : records) {
                log.info("Received message topic {} : ({}, {}) at partition {} offset {}", record.topic(), record.key(), record.value(), record.partition(), record.offset());
            }
            count += records.count(); // 记录累加
            if (records.count() == 0) {
                num--;
                if (num < 0) {
                    break;
                }
            }
        }
        log.info("poll topic {} size {} time {} ms", topic, count, System.currentTimeMillis() - start);
    }
}
```

## 启动连接器 Worker

- 修改 kafka 文件夹下 config/connect-distributed.properties 文件增加以下配置
```properties
# rest 请求地址
rest.host.name=localhost
rest.port=8083
# 插件地址,为了清楚区分插件的 jar 和 kafka 的 jar 在 kafka 下增加插件文件夹（plugins），启动连接器时会加载该目录下的 jar。
# 如果没有配置这个路径需要把插件的所有jar复制到 kafka 的 libs 目录下
plugin.path=/software/kafka_2.12-2.3.0/plugins
```

- 启动连接器 Worker, 启动时有类着不到警告，暂时没有找到原因
```
bin/windows/connect-distributed.bat config/connect-distributed.properties 
```

- 查看所有的连接器插件 `curl localhost:8083/connector-plugins`. 会有两个连接器，这两个是 kafka 自带的
```json
[{
    "class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
    "type": "sink",
    "version": "2.3.0"
}, {
    "class": "org.apache.kafka.connect.file.FileStreamSourceConnector",
    "type": "source",
    "version": "2.3.0"
}]
```

官方文档中有针对这两个连接器的简单用例，可以参考文档试一下。这里就不详细介绍了，下一篇文章会介绍如何利用kafka连接器同步 postgres 数据到 elasticsearch 中

## 参考地址
- [java 客户端源码地址](https://github.com/yupengj/kafka-examples)
- [kafka 快速开始官网文档](http://kafka.apache.org/quickstart)
- [kafka 连接器官网文档](http://kafka.apache.org/documentation/#connect)